import cv2
import threading
import numpy as np
import platform
import emoji
from PIL import Image, ImageDraw, ImageFont
from deepface import DeepFace

from utils.action_classifier import get_classifier
from utils.pose_estimation import get_pose_estimator
from utils.tracker import get_tracker
from utils.utils.config import Config
from utils.utils.drawer import Drawer
from utils.utils.utils import convert_to_openpose_skeletons

class ActionDetectionSystem:
    def __init__(self, config_path="config.yaml", max_objects=8):
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = Config(config_path)
        self.current_objects = []
        self.initialize_models()
        self.frame_lock = threading.Lock()
        self.max_objects = max_objects
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
        self.action_names = {
            'stand': '–°—Ç–æ–∏—Ç',
            'walk': '–•–æ–¥—å–±–∞',
            'run': '–ë–µ–≥',
            'jump': '–ü—Ä—ã–∂–æ–∫',
            'sit': '–°–∏–¥–∏—Ç',
            'squat': '–ü—Ä–∏—Å–µ–¥–∞–µ—Ç',
            'kick': '–£–¥–∞—Ä –Ω–æ–≥–æ–π',
            'punch': '–£–¥–∞—Ä —Ä—É–∫–æ–π',
            'wave': '–ú–∞—Ö–∞–Ω–∏–µ',
            'unknown': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
        }
    
    def initialize_models(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self.pose_estimator = get_pose_estimator(**self.config.POSE)
        self.tracker = get_tracker(**self.config.TRACKER)
        self.action_classifier = get_classifier(**self.config.CLASSIFIER)
        self.drawer = Drawer()
    
    def predict_poses(self, rgb_frame):
        predictions = self.pose_estimator.predict(rgb_frame, get_bbox=True)
        if len(predictions) == 0:
            self.tracker.increment_ages()
            return []
        return self.limit_predictions(predictions)
    
    def limit_predictions(self, predictions):
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—ä–µ–∫—Ç–æ–≤
        if len(predictions) > self.max_objects:
            predictions = predictions[:self.max_objects]
        return predictions
    
    def normalize_bboxes(self, predictions):
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö bbox –≤ numpy –º–∞—Å—Å–∏–≤—ã
        for pred in predictions:
            if hasattr(pred, "bbox") and pred.bbox is not None:
                if not isinstance(pred.bbox, np.ndarray):
                    pred.bbox = np.array(pred.bbox)
        return predictions
    
    def track_objects(self, rgb_frame, predictions):
        predictions = convert_to_openpose_skeletons(predictions)
        
        try:
            predictions, _ = self.tracker.predict(rgb_frame, predictions)
            return predictions
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Ç—Ä–µ–∫–µ—Ä–∞: {e}")
            self.tracker.increment_ages()
            return predictions
    
    def classify_actions(self, predictions):
        if len(predictions) == 0:
            return predictions
            
        try:
            predictions = self.action_classifier.classify(predictions)
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π
            for pred in predictions:
                if hasattr(pred, 'action') and pred.action and pred.action[0]:
                    action_name = pred.action[0]
                    confidence = pred.action[1]
                    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ —Ä—É—Å—Å–∫–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ)
                    translated_name = self.action_names.get(action_name, action_name)
                    pred.action = (translated_name, confidence)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {e}")
        
        return predictions
    
    def render_frame(self, frame, predictions):
        try:
            return self.drawer.render_frame(
                frame, 
                predictions, 
                text_color='green', 
                add_blank=False
            )
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞: {e}")
            return frame
        
    def get_current_objects(self):
        objects_data = []
        with self.frame_lock:
            for pred in self.current_objects:
                try:
                    track_id = getattr(pred, 'id', 'N/A')
                    action_data = getattr(pred, 'action', ['unknown', 0.0])

                    obj = {
                        "id": track_id,
                        "action": str(action_data[0]),
                        "confidence": float(action_data[1])
                    }
                    objects_data.append(obj)
                except Exception as e:
                    print(f"Error processing prediction: {e}")
        return objects_data
    
    def process_frame(self, frame):
        if frame is None:
            return frame
            
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–∑—ã
        predictions = self.predict_poses(frame)
        if len(predictions) == 0:
            with self.frame_lock:
                self.current_objects = []
            return frame
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è bbox
        predictions = self.normalize_bboxes(predictions)
        
        # –¢—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤
        predictions = self.track_objects(frame, predictions)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏–π
        predictions = self.classify_actions(predictions)
        
        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è bbox –ø–µ—Ä–µ–¥ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º
        predictions = self.normalize_bboxes(predictions)

        with self.frame_lock:
            self.current_objects = predictions
        
        # –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –∫–∞–¥—Ä–∞
        return self.render_frame(frame, predictions)

def generate_frames(system, camera_id=0):
    cap = cv2.VideoCapture(camera_id)
    while True:
        ret, frame = cap.read()
        if not ret:
            print("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –∫–∞–¥—Ä")
            break
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ —Å–∏—Å—Ç–µ–º–æ–π
        processed_frame = system.process_frame(frame)
        
        ret, buffer = cv2.imencode('.jpg', processed_frame)
        frame_data = buffer.tobytes()
        yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' + frame_data + b'\r\n')

class EmotionDetectionSystem:
    def __init__(self, config_path="config.yaml"):
        self.face_cascade = cv2.CascadeClassifier("haarcascade_frontalface_default.xml")
        self.drawer = Drawer()
        self.emotion_data = {
            "happy": {
                "text": "–°—á–∞—Å—Ç—å–µ",
                "emoji": "üòä",
                "bgr": (0, 255, 0),    # –ó–µ–ª–µ–Ω—ã–π –¥–ª—è —Ä–∞–º–∫–∏ (BGR)
                "rgb": (0, 255, 0)     # –ó–µ–ª–µ–Ω—ã–π –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (RGB)
            },
            "sad": {
                "text": "–ì—Ä—É—Å—Ç—å",
                "emoji": "üò¢",
                "bgr": (0, 0, 255),    # –ö—Ä–∞—Å–Ω—ã–π –¥–ª—è —Ä–∞–º–∫–∏ (BGR)
                "rgb": (255, 0, 0)     # –ö—Ä–∞—Å–Ω—ã–π –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (RGB)
            },
            "angry": {
                "text": "–ó–ª–æ—Å—Ç—å",
                "emoji": "üò†",
                "bgr": (255, 0, 0),    # –°–∏–Ω–∏–π –¥–ª—è —Ä–∞–º–∫–∏ (BGR)
                "rgb": (0, 0, 255)     # –°–∏–Ω–∏–π –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (RGB)
            },
            "surprise": {
                "text": "–£–¥–∏–≤–ª–µ–Ω–∏–µ",
                "emoji": "üò≤",
                "bgr": (0, 255, 255),  # –ñ–µ–ª—Ç—ã–π –¥–ª—è —Ä–∞–º–∫–∏ (BGR)
                "rgb": (255, 255, 0)   # –ñ–µ–ª—Ç—ã–π –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (RGB)
            },
            "neutral": {
                "text": "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ",
                "emoji": "üòê",
                "bgr": (128, 128, 128),# –°–µ—Ä—ã–π –¥–ª—è —Ä–∞–º–∫–∏ (BGR)
                "rgb": (128, 128, 128) # –°–µ—Ä—ã–π –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (RGB)
            }
        }
        self.current_objects = []
        self.frame_lock = threading.Lock()

    def process_frame(self, frame):
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –æ—Ç—Ç–µ–Ω–∫–∏ —Å–µ—Ä–æ–≥–æ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª–∏—Ü
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ª–∏—Ü
            faces = self.face_cascade.detectMultiScale(
                gray, 
                scaleFactor=1.3,
                minNeighbors=5,
                minSize=(50, 50)
            )
            
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–∏–∫—Ü–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å Drawer
            predictions = []
            
            for i, (x, y, w, h) in enumerate(faces):
                # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–∏
                emotion = self._process_face(frame, x, y, w, h)
                if not emotion:
                    continue
                    
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —ç–º–æ—Ü–∏–∏
                emotion_info = self.emotion_data.get(emotion, self.emotion_data["neutral"])
                
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –ø—Ä–µ–¥–∏–∫—Ü–∏–∏
                from types import SimpleNamespace
                pred = SimpleNamespace()
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º keypoints = [] –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Drawer
                pred.keypoints = np.zeros((18, 3), dtype=np.int16)
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º bbox
                pred.bbox = np.array([x, y, x+w, y+h])
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ID
                pred.id = i + 1
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —ç–º–æ—Ü–∏—é
                pred.emotion = emotion_info["text"]
                pred.emotion_score = 0.95
                pred.emotion_emoji = emotion_info["emoji"]
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ü–≤–µ—Ç (–≤ BGR)
                pred.color = emotion_info["bgr"]
                
                predictions.append(pred)
            
            with self.frame_lock:
                self.current_objects = predictions
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º drawer –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
            if predictions:
                return self.drawer.render_frame(frame, predictions)
            else:
                return frame
            
        except Exception as e:
            print(f"Emotion processing error: {e}")
            return frame

    def _process_face(self, frame, x, y, w, h):
        try:
            face_roi = frame[y:y+h, x:x+w]
            result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)
            return result[0]['dominant_emotion']
        except Exception as e:
            print("–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–∏:", e)
            return None

    def get_current_objects(self):
        objects_data = []
        with self.frame_lock:
            for pred in self.current_objects:
                objects_data.append({
                    "id": pred.id,
                    "emotion": getattr(pred, 'emotion', 'unknown'),
                    "confidence": getattr(pred, 'emotion_score', 0.0)
                })
        return objects_data